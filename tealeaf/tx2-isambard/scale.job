#!/bin/bash
#PBS -q arm
#PBS -l walltime=24:00:00
#PBS -joe

set -u

cd "$PBS_O_WORKDIR"

cp "$SRC_DIR/Benchmarks/tea_bm_5x2.in" tea.in

export OMP_WAIT_POLICY=active

# Set Cray recommended flags
export MPICH_RANK_REORDER_METHOD=4
export MPICH_RANK_REORDER_OPTS="--ndims=2 --dims=8000,8000"
export MPICH_USE_DMAPP_COLL=1
export MPICH_GNI_MAX_VSHORT_MSG_SIZE=8192

# Try all combinations, including hyperthreads


# Flat MPI, with SMT via MPI
export OMP_NUM_THREADS=1
for SMT in {1..4}; do
  aprun -n $[NODES * 64 * SMT] -N $[64 * SMT] -d 1 -j $SMT -cc depth "$CFG_DIR/$BENCHMARK_EXE"
  mv $CFG_DIR/tea.out $CFG_DIR/tea.$NODES.$[NODES * 64 * SMT].$OMP_NUM_THREADS.$SMT.out
done

# 1 MPI per node
for SMT in {1..4}; do
  export OMP_NUM_THREADS=$[64 * SMT]
  aprun -n $[NODES] -N 1 -d $OMP_NUM_THREADS -j $SMT -c depth "$CFG_DIR/$BENCHMARK_EXE"
  mv $CFG_DIR/tea.out $CFG_DIR/tea.$NODES.1.$OMP_NUM_THREADS.$SMT.out
done

# Hybrid runs
for MPIS in 2 4 8 16 32; do
  OMPS=$[64 / MPIS]
  for SMT in {1..4}; do
    export OMP_NUM_THREADS=$[OMPS * SMT]
    aprun -n $[NODES * MPIS] -N $MPIS -S $[MPIS / 2] -d $OMP_NUM_THREADS -j $SMT -cc depth "$CFG_DIR/$BENCHMARK_EXE"
    mv $CFG_DIR/tea.out $CFG_DIR/tea.$NODES.$[NODES * MPIS].$OMP_NUM_THREADS.$SMT.out
  done
done



